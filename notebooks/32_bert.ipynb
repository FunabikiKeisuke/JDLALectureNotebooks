{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910756f7-12ef-4246-96f6-8008a48ca044",
   "metadata": {},
   "source": [
    "# Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 目的\n",
    "Bidirectional Encoder Representations from Transformers (BERT) を理解する．\n",
    "\n",
    "\n",
    "\n",
    "## モジュールのインポート・データのダウンロード\n",
    "演習に使用するモジュールとデータをダウンロードします．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f699d960-e64f-44c2-aeb8-00dbaa483d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers fugashi ipadic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "26044fb5-695a-4859-88c1-d26a8c4fa59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import tarfile\n",
    "\n",
    "# GPUが使えれば利用する設定\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fa0166-4a8d-44e5-943a-7cbd14019278",
   "metadata": {},
   "source": [
    "## データセットの作成\n",
    "\n",
    "ダウンロードしたデータを読み込んで，学習データの作成を行います．\n",
    "\n",
    "今回使用するデータは，[livedoorニュースコーパスデータセット](https://www.rondhuit.com/download.html#ldcc)です．\n",
    "これは，Web上のweb記事のデータセットとなっており，複数のカテゴリのニュース記事から構成されています．\n",
    "今回はこのうち，「ITLife Hack（IT関連記事）」と「Sports Watch（スポーツ関連記事）」を分類するタスクおよびデータセットを扱います．\n",
    "\n",
    "まずデータをダウンロードし，実験に使用するデータ飲みを抽出し，テキストファイルに一度書き出します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7790005f-b5c4-4527-bbe1-03de3a38d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgz_fname = \"ldcc-20140209.tar.gz\"              # ダウンロードした圧縮ファイルのパスを設定\n",
    "target_genre = [\"it-life-hack\", \"sports-watch\"] # 2つをニュースメディアのジャンルを選定\n",
    "tsv_fname = \"all_text.tsv\"                      # 処理をした結果を保存するファイル名 \n",
    "\n",
    "# データのダウンロード（カレントディレクトリに圧縮ファイルがダウンロードされる）\n",
    "urllib.request.urlretrieve(\"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\", \"ldcc-20140209.tar.gz\")\n",
    "\n",
    "# 処理部分 -------\n",
    "brackets_tail = re.compile('【[^】]*】$')\n",
    "brackets_head = re.compile('^【[^】]*】')\n",
    "\n",
    "def remove_brackets(inp):\n",
    "    output = re.sub(brackets_head, '', re.sub(brackets_tail, '', inp))\n",
    "    return output\n",
    "\n",
    "def read_title(f):\n",
    "    # 2行スキップ\n",
    "    next(f)\n",
    "    next(f)\n",
    "    title = next(f) # 3行目を返す\n",
    "    title = remove_brackets(title.decode('utf-8'))\n",
    "    return title[:-1]\n",
    "\n",
    "zero_fnames = []\n",
    "one_fnames = []\n",
    "\n",
    "with tarfile.open(tgz_fname) as tf:\n",
    "    # 対象ファイルの選定\n",
    "    for ti in tf:\n",
    "        # ライセンスファイルはスキップ\n",
    "        if \"LICENSE.txt\" in ti.name:\n",
    "            continue\n",
    "        if target_genre[0] in ti.name and ti.name.endswith(\".txt\"):\n",
    "            zero_fnames.append(ti.name)\n",
    "            continue\n",
    "        if target_genre[1] in ti.name and ti.name.endswith(\".txt\"):\n",
    "            one_fnames.append(ti.name)\n",
    "    with open(tsv_fname, \"w\") as wf:\n",
    "        writer = csv.writer(wf, delimiter='\\t')\n",
    "        # ラベル 0\n",
    "        for name in zero_fnames:\n",
    "            f = tf.extractfile(name)\n",
    "            title = read_title(f)\n",
    "            row = [target_genre[0], 0, '', title]\n",
    "            writer.writerow(row)\n",
    "        # ラベル 1\n",
    "        for name in one_fnames:\n",
    "            f = tf.extractfile(name)\n",
    "            title = read_title(f)\n",
    "            row = [target_genre[1], 1, '', title]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccf266-4ec7-4fd6-a074-834a80eec3d9",
   "metadata": {},
   "source": [
    "### データの表示と確認\n",
    "\n",
    "抽出したデータを読み込み，一部を表示して内容を確認します．\n",
    "\n",
    "表示すると，文章と記事のクラスなどがまとめられていることが確認できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a24d6671-3fb9-4c6b-9473-8f3833a08fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データサイズ： (1770, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>media_name</th>\n",
       "      <th>label</th>\n",
       "      <th>NaN</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>sports-watch</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>山崎明かす、予告ホームランは「5回くらいやってます」</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>it-life-hack</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>いつでもどこでも自分専用環境！Windowsトラブル時にも使えるUbuntu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>sports-watch</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>浅尾美和、新パートナーとはやくもちぐはぐ!?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>sports-watch</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>巨人の拙劣な采配に批判殺到、ノムさんも「根拠がサッパリわからない」</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>it-life-hack</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>アイコンを変更してフォルダーを目立たせる</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>sports-watch</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>オリックス公式アカウント「ヘディング脳」発言が波紋</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>sports-watch</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ザック・ジャパンの″秘密兵器″報道に賛否</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>sports-watch</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>原監督の采配皮肉る落合氏にファンも同調</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>it-life-hack</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>高ワットを安定供給できる電源！　ZALMANより1250ワット電源発売開始</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>sports-watch</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>澤、今後を語る「指導者は絶対ないですね」</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        media_name  label  NaN                                sentence\n",
       "1536  sports-watch      1  NaN              山崎明かす、予告ホームランは「5回くらいやってます」\n",
       "547   it-life-hack      0  NaN  いつでもどこでも自分専用環境！Windowsトラブル時にも使えるUbuntu\n",
       "1147  sports-watch      1  NaN                  浅尾美和、新パートナーとはやくもちぐはぐ!?\n",
       "1647  sports-watch      1  NaN       巨人の拙劣な采配に批判殺到、ノムさんも「根拠がサッパリわからない」\n",
       "200   it-life-hack      0  NaN                    アイコンを変更してフォルダーを目立たせる\n",
       "1488  sports-watch      1  NaN               オリックス公式アカウント「ヘディング脳」発言が波紋\n",
       "1542  sports-watch      1  NaN                    ザック・ジャパンの″秘密兵器″報道に賛否\n",
       "1649  sports-watch      1  NaN                     原監督の采配皮肉る落合氏にファンも同調\n",
       "846   it-life-hack      0  NaN   高ワットを安定供給できる電源！　ZALMANより1250ワット電源発売開始\n",
       "1334  sports-watch      1  NaN                    澤、今後を語る「指導者は絶対ないですね」"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データの読み込み\n",
    "df = pd.read_csv(\"all_text.tsv\", \n",
    "                 delimiter='\\t', header=None, names=['media_name', 'label', 'NaN', 'sentence'])\n",
    "\n",
    "# データの確認\n",
    "print(f'データサイズ： {df.shape}')\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39ade6a-ca47-4b57-bde0-c3a067aca467",
   "metadata": {},
   "source": [
    "### データの抽出\n",
    "\n",
    "上記では，pandasのDataFrameでデータを読み込んでいるため，\n",
    "必要なデータのみをDataFrameから抽出します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "793aba4c-2aa0-4d24-ad3c-29333ef0d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの抽出\n",
    "sentences = df.sentence.values\n",
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da65742-9095-4a8d-974e-e34a643451dd",
   "metadata": {},
   "source": [
    "### 日本語文章の分解とIDへの変換\n",
    "\n",
    "次に，データセットの日本語文章を単語に分解し，各単語に対応するIDへ変換を行います\n",
    "\n",
    "この処理には，`BertJapaneseTokenizer`を用いて変換します．\n",
    "\n",
    "まず，`BertJapaneseTokenizer`のインスタンスを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d201110-e084-4e3c-937b-add9bba946f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertJapaneseTokenizer\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debb022-8e6b-45d0-aaa8-b7286f616b1c",
   "metadata": {},
   "source": [
    "次に作成したTokenizerを使用して，日本語文章を単語およびIDへの変換結果を確認してみます．\n",
    "\n",
    "単語に分解されていることが確認できます．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72e74348-5e70-4905-b7d5-fba174822695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  旧式Macで禁断のパワーアップ！最新PCやソフトを一挙にチェック\n",
      "Tokenized:  ['旧式', 'Mac', 'で', '禁', '##断', 'の', 'パワーアップ', '!', '最新', 'PC', 'や', 'ソフト', 'を', '一挙', 'に', 'チェック']\n",
      "Token IDs:  [18718, 8653, 12, 1763, 29135, 5, 20734, 679, 6215, 3794, 49, 1604, 11, 24598, 7, 9398]\n"
     ]
    }
   ],
   "source": [
    "# 元文章\n",
    "print('Original: ', sentences[0])\n",
    "# Tokenizer\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[0]))\n",
    "# Token-id\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89154a7a-ef1d-43b8-b2df-164cc47e4227",
   "metadata": {},
   "source": [
    "次にデータセット全ての文章を単語へ分解しID変換したデータを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57f54248-3c8a-4364-bf8c-0773c2a9d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大単語数:  35\n",
      "注意：上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値が最大単語数\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    }
   ],
   "source": [
    "# 最大単語数の確認\n",
    "max_len = []\n",
    "\n",
    "# 1文づつ処理\n",
    "for sent in sentences:\n",
    "    # Tokenizeで分割\n",
    "    token_words = tokenizer.tokenize(sent)\n",
    "    # 文章数を取得してリストへ格納\n",
    "    max_len.append(len(token_words))\n",
    "\n",
    "# 最大の値を確認\n",
    "print('最大単語数:', max(max_len))\n",
    "print('注意：上記の最大単語数にSpecial token（[CLS], [SEP]）の+2をした値が最大単語数')\n",
    "\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "# 1文づつ処理\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      \n",
    "                        add_special_tokens = True,      # Special Tokenの追加\n",
    "                        max_length = 37,                # 文章の長さを固定（Padding/Trancatinating）\n",
    "                        pad_to_max_length = True,       # PADDINGで埋める\n",
    "                        return_attention_mask = True,   # Attention maksの作成\n",
    "                        return_tensors = 'pt',          #  Pytorch tensorsで返す\n",
    "                   )\n",
    "\n",
    "    # 単語IDを取得    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "\n",
    "    # Attention maskの取得\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# リストに入ったtensorを縦方向（dim=0）へ結合\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# tenosor型に変換\n",
    "labels = torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449985e2-9bb8-45d5-8c7e-1c45f004100e",
   "metadata": {},
   "source": [
    "### データセットクラスの作成\n",
    "\n",
    "読み込み・整理をしたデータを用いて，データセットクラスを作成します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c41c13dc-86b0-459c-a3fc-4e8c5ed4f8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練データ数: 1593\n",
      "検証データ数: 177\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# データセットクラスの作成\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# 90%地点のIDを取得\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# データセットを分割\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('訓練データ数: {}'.format(train_size))\n",
    "print('検証データ数: {}'.format(val_size))\n",
    "\n",
    "# データローダーの作成\n",
    "batch_size = 32\n",
    "\n",
    "# 訓練データローダー\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  \n",
    "            sampler = RandomSampler(train_dataset),   # ランダムにデータを取得してバッチ化\n",
    "            batch_size = batch_size\n",
    "        )\n",
    "\n",
    "# 検証データローダー\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            sampler = SequentialSampler(val_dataset), # 順番にデータを取得してバッチ化\n",
    "            batch_size = batch_size\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a68100-ff89-4f2f-a4c6-58882b935306",
   "metadata": {},
   "source": [
    "## ネットワークモデルの作成\n",
    "\n",
    "ネットワークモデルを作成します．\n",
    "\n",
    "今回は，BERTの日本語用のPre-trainedモデルを活用し，Fine-Tuningを行うことで，記事の分類を行います．\n",
    "\n",
    "Pre-trainedモデルの読み込みには`BertForSequenceClassification`を活用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "098d4525-5958-440a-a176-8fe57a43cfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# BertForSequenceClassification 学習済みモデルのロード\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"cl-tohoku/bert-base-japanese-whole-word-masking\", # 日本語Pretrainedモデルの指定\n",
    "    num_labels = 2,                                    # ラベル数（今回はBinayなので2、数値を増やせばマルチラベルも対応可）\n",
    "    output_attentions = False,                         # アテンションベクトルを出力するか\n",
    "    output_hidden_states = False,                      # 隠れ層を出力するか\n",
    ")\n",
    "\n",
    "# モデルをGPUへ転送\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae4a8ba-0de3-4654-8751-a87e1e5efc45",
   "metadata": {},
   "source": [
    "## 学習\n",
    "ネットワークを学習（Fine-Tuning）します．\n",
    "\n",
    "最適化手法には，Adamに用いるweight decayを改良した`AdamW`を使用します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "806a2eaa-3ae0-405c-8ab4-1503f2ece0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# 訓練パートの定義\n",
    "def train(model):\n",
    "    model.train() # 訓練モードで実行\n",
    "    train_loss = 0\n",
    "    for batch in train_dataloader:# train_dataloaderはword_id, mask, labelを出力する点に注意\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels).loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    return train_loss\n",
    "\n",
    "# テストパートの定義\n",
    "def validation(model):\n",
    "    model.eval()# 訓練モードをオフ\n",
    "    val_loss = 0\n",
    "    with torch.no_grad(): # 勾配を計算しない\n",
    "        for batch in validation_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "            with torch.no_grad():\n",
    "                loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask,labels=b_labels).loss\n",
    "            val_loss += loss.item()\n",
    "    return val_loss\n",
    "\n",
    "# 学習の実行\n",
    "max_epoch = 4\n",
    "train_loss_ = []\n",
    "test_loss_ = []\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    train_ = train(model)\n",
    "    test_ = train(model)\n",
    "    train_loss_.append(train_)\n",
    "    test_loss_.append(test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27494c56-c45e-447c-86d8-3cdc85f30063",
   "metadata": {},
   "source": [
    "## 検証\n",
    "\n",
    "学習したモデルを検証データで評価します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9a1e67fd-effa-4950-aa90-960767fadf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.4745762711864407\n"
     ]
    }
   ],
   "source": [
    "# 検証方法の確認（1バッチ分で計算ロジックに確認）\n",
    "count = 0\n",
    "model.eval()# 訓練モードをオフ\n",
    "for batch in validation_dataloader:\n",
    "    b_input_ids = batch[0].to(device)\n",
    "    b_input_mask = batch[1].to(device)\n",
    "    b_labels = batch[2].to(device)\n",
    "    with torch.no_grad():   \n",
    "        # 学習済みモデルによる予測結果をpredsで取得     \n",
    "        preds = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        if count == 0:\n",
    "            PRED = preds[0]\n",
    "            LABEL = b_labels\n",
    "        else:\n",
    "            PRED = torch.cat((PRED, preds[0]), dim=0)\n",
    "            LABEL = torch.cat((LABEL, b_labels), dim=0)\n",
    "        count += 1\n",
    "        \n",
    "\n",
    "# 比較しやすい様にpd.dataframeへ整形\n",
    "# pd.dataframeへ変換（GPUに乗っているTensorはgpu->cpu->numpy->dataframeと変換）\n",
    "logits_df = pd.DataFrame(PRED.cpu().numpy(), columns=['logit_0', 'logit_1'])\n",
    "\n",
    "# np.argmaxで大き方の値を取得\n",
    "pred_df = pd.DataFrame(np.argmax(PRED.cpu().numpy(), axis=1), columns=['pred_label'])\n",
    "label_df = pd.DataFrame(LABEL.cpu().numpy(), columns=['true_label'])\n",
    "\n",
    "print(\"accuracy: \", np.sum(np.argmax(PRED.cpu().numpy(), axis=1) == LABEL.cpu().numpy()) / len(PRED))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dc59cb-c5f8-428b-9d5a-7557e3d98432",
   "metadata": {},
   "source": [
    "最後に結果をDataFrameで一覧表示し，確認します．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5505b681-d83c-4853-a015-df89de118b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logit_0</th>\n",
       "      <th>logit_1</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>true_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.476354</td>\n",
       "      <td>-0.056060</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.180653</td>\n",
       "      <td>0.148272</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.286365</td>\n",
       "      <td>0.200451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.101160</td>\n",
       "      <td>-0.108817</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.178711</td>\n",
       "      <td>0.061986</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.305080</td>\n",
       "      <td>0.079071</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.267367</td>\n",
       "      <td>0.134212</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.162382</td>\n",
       "      <td>-0.067926</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.314368</td>\n",
       "      <td>0.100784</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.161507</td>\n",
       "      <td>0.027042</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      logit_0   logit_1  pred_label  true_label\n",
       "0    0.476354 -0.056060           0           1\n",
       "1    0.180653  0.148272           0           1\n",
       "2    0.286365  0.200451           0           0\n",
       "3    0.101160 -0.108817           0           1\n",
       "4    0.178711  0.061986           0           1\n",
       "..        ...       ...         ...         ...\n",
       "172  0.305080  0.079071           0           1\n",
       "173  0.267367  0.134212           0           1\n",
       "174  0.162382 -0.067926           0           1\n",
       "175  0.314368  0.100784           0           0\n",
       "176  0.161507  0.027042           0           1\n",
       "\n",
       "[177 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df = pd.concat([logits_df, pred_df, label_df], axis=1)\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c8ed40-976a-40de-b43b-13ad912ff7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
